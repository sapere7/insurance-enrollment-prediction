# Configuration for the Insurance Enrollment Prediction Pipeline

# General Settings
random_state: 42
test_size: 0.2

# File Paths
data_file: 'employee_data.csv'
output_dir: 'output'
figures_dir: 'output/figures'
models_dir: 'output/models'
tuning_dir: 'output/tuning'
processed_data_dir: 'output/processed_data' # New directory for processed data

# EDA Settings
eda:
  age_bins: [18, 25, 35, 45, 55, 65, 100]
  salary_bins: [0, 50000, 75000, 100000, 150000, 200000, 1000000]
  tenure_bins: [0, 1, 3, 5, 10, 50]

# Model Settings
models:
  LogisticRegression:
    max_iter: 5000 # Increased further to potentially avoid ConvergenceWarning with sag/saga
    # random_state will be applied from global config if not specified here
  RandomForest:
    n_estimators: 100
    # random_state will be applied from global config if not specified here
  GradientBoosting:
    n_estimators: 100
    # random_state will be applied from global config if not specified here

# Model Selection Weights (for main.py)
selection_weights:
  roc_auc: 0.4
  f1: 0.3
  recall: 0.3

# Hyperparameter Tuning Settings
tuning:
  cv_folds: 5
  random_search_iterations: 50 # Number of iterations for RandomizedSearchCV
  scoring_metric: 'roc_auc'

  # Logistic Regression Tuning Grid
  logreg:
    param_grid: # Using list of dicts for valid combinations
      - model__solver: ['liblinear']
        model__penalty: ['l1', 'l2']
        model__C: [0.001, 0.01, 0.1, 1, 10, 100]
        model__class_weight: [null, 'balanced']
      - model__solver: ['lbfgs', 'newton-cg', 'sag', 'saga'] # saga supports l1/l2/elasticnet/None
        model__penalty: ['l2', null] # lbfgs, newton-cg, sag support only l2/None
        model__C: [0.001, 0.01, 0.1, 1, 10, 100]
        model__class_weight: [null, 'balanced']
        # Note: saga also supports l1, but separating for clarity
      - model__solver: ['saga']
        model__penalty: ['l1']
        model__C: [0.001, 0.01, 0.1, 1, 10, 100]
        model__class_weight: [null, 'balanced']
    # Add elasticnet for saga if needed, requires l1_ratio
    # - model__solver: ['saga']
    #   model__penalty: ['elasticnet']
    #   model__C: [0.001, 0.01, 0.1, 1, 10, 100]
    #   model__l1_ratio: [0.1, 0.5, 0.9]
    #   model__class_weight: [null, 'balanced']

  # Random Forest Tuning Grid (Randomized Search)
  rf:
    param_dist:
      model__n_estimators: [100, 200, 300, 400, 500]
      model__max_features: ['sqrt', 'log2', null] # Replaced 'auto' with 'sqrt'/'log2'
      model__max_depth: [5, 10, 15, 20, null]
      model__min_samples_split: [2, 5, 8, 10]
      model__min_samples_leaf: [1, 3, 5, 7]
      model__bootstrap: [True, False]
      model__class_weight: [null, 'balanced', 'balanced_subsample']

    # Focused Grid Search (around best params from random search) - Define ranges here
    focused_grid:
      model__n_estimators: [200, 250, 300] # Example: Center around best n_estimators found
      model__max_features: ['sqrt'] # Example: If 'sqrt' was best
      model__max_depth: [13, 15, 17] # Example: Center around best max_depth
      model__min_samples_split: [6, 8, 10] # Example: Center around best min_samples_split
      model__min_samples_leaf: [3, 4, 5] # Example: Center around best min_samples_leaf
      model__class_weight: [null] # Example: If null was best

  # Gradient Boosting Tuning Grid (Randomized Search)
  gb:
    param_dist:
      model__n_estimators: [100, 200, 300, 400, 500]
      model__learning_rate: [0.01, 0.05, 0.1, 0.2, 0.3]
      model__max_depth: [2, 3, 4, 5]
      model__min_samples_split: [2, 5, 10]
      model__min_samples_leaf: [1, 3, 5]
      model__subsample: [0.7, 0.8, 0.9, 1.0]

    # Focused Grid Search (around best params from random search) - Define ranges here
    focused_grid:
      model__n_estimators: [180, 210, 240] # Example
      model__learning_rate: [0.2, 0.25, 0.3] # Example
      model__max_depth: [2] # Example
      model__min_samples_split: [3, 4, 5] # Example
      model__min_samples_leaf: [7, 8, 9] # Example
      model__subsample: [0.85, 0.9, 0.95] # Example
